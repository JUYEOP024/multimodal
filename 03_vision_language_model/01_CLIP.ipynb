{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace5f359",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "\n",
    "<img src=\"https://d.pr/i/zcPHYr+\" width=500/>\n",
    "\n",
    "https://huggingface.co/openai/models?search=clip-vit\n",
    "\n",
    "https://huggingface.co/openai/clip-vit-base-patch32\n",
    "\n",
    "CLIP 모델은 **OpenAI**에서 개발한 **멀티모달 AI 모델**로, **이미지와 텍스트를 동시에 이해**하고 연결하는 능력을 갖춘 모델이다. CLIP의 이름은 \"Contrastive Language-Image Pre-training\"의 약자이다.\n",
    "\n",
    "- **구성**  CLIP은 두 개의 신경망으로 구성된다.  \n",
    "  - **Vision Encoder(이미지 인코더)**: 이미지 데이터를 벡터로 변환. Vision Transformer(ViT) 구조를 사용한다. (초기에는 ResNet CNN 사용)\n",
    "  - **Text Encoder(텍스트 인코더)**: 텍스트 데이터를 벡터로 변환. Transformer 기반의 언어 모델\n",
    "  이 두 인코더는 각각의 입력(이미지, 텍스트)을 **같은 차원의 임베딩 공간**에 매핑한다.\n",
    "\n",
    "- **학습 방식**  \n",
    "  \n",
    "  <img src=\"https://viso.ai/wp-content/uploads/2024/04/vision-language-models-clip-1280x845.jpg\" width=500/>\n",
    "  \n",
    "  CLIP은 **이미지-텍스트 쌍** 데이터(예: 사진과 그 설명)를 대규모로 수집하여, 같은 쌍의 임베딩은 **가깝게**, 다른 쌍은 **멀게** 학습하는 **대조적 학습(Contrastive Learning)** 방식을 사용한다. 배치 내 모든 이미지-텍스트 쌍에 대해 코사인 유사도를 계산하고, 올바른 쌍의 유사도를 최대화, 잘못된 쌍의 유사도를 최소화하는 **loss**를 사용한다.\n",
    "\n",
    "- **주요 장점**  \n",
    "  - **Zero-shot 학습**: 별도의 추가 학습 없이도, 텍스트 설명만으로 다양한 이미지 분류나 검색이 가능하다. 예를 들어, \"A photo of a cat\"과 같은 문장을 입력하면, 해당 설명에 가장 잘 맞는 이미지를 찾을 수 있다.\n",
    "  - **범용성**: 이미지 분류, 검색, 생성 등 다양한 태스크에 적용할 수 있다.\n",
    "  - **대규모 데이터 활용**: 웹에서 수집한 4억 쌍의 이미지-텍스트 데이터를 사용해 사전학습함으로써, 기존 모델보다 일반화 성능이 뛰어나다.\n",
    "\n",
    "- **활용 예시**  \n",
    "  - 이미지 검색: \"노을지는 바다\"라는 텍스트로 관련 이미지를 찾기  \n",
    "  - 이미지 분류: 텍스트 라벨로 다양한 이미지를 분류  \n",
    "  - 생성 AI의 멀티모달 입력 처리 등\n",
    "\n",
    "CLIP의 핵심 아이디어는 **이미지와 텍스트를 같은 임베딩 공간에 위치시키는 것**이며, 이를 통해 두 다른 형태의 데이터를 효과적으로 연결할 수 있다.  \n",
    "이런 구조 덕분에 CLIP은 **특정 태스크에 맞춰 추가로 학습하지 않아도 다양한 문제를 해결**할 수 있다.\n",
    "\n",
    "수식 예시(코사인 유사도):\n",
    "\n",
    "$\n",
    "\\text{similarity}(v_{\\text{image}}, v_{\\text{text}}) = \\frac{v_{\\text{image}} \\cdot v_{\\text{text}}}{\\|v_{\\text{image}}\\| \\|v_{\\text{text}}\\|}\n",
    "$\n",
    "\n",
    "여기서 $v_{\\text{image}}$와 $v_{\\text{text}}$는 각각 이미지와 텍스트의 임베딩 벡터이다.\n",
    "\n",
    "정리하면, **CLIP은 이미지와 텍스트를 동시에 이해하고 연결하는 범용 멀티모달 AI 모델**로, 다양한 분야에서 강력한 zero-shot 성능과 범용성을 보여주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731f7615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73fe9255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1e935149bb493eaaaf587948f14e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPImageProcessorFast {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "CLIPTokenizer(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, added_tokens_decoder={\n",
      "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "CLIPModel(\n",
      "  (text_model): CLIPTextTransformer(\n",
      "    (embeddings): CLIPTextEmbeddings(\n",
      "      (token_embedding): Embedding(49408, 512)\n",
      "      (position_embedding): Embedding(77, 512)\n",
      "    )\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (vision_model): CLIPVisionTransformer(\n",
      "    (embeddings): CLIPVisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
      "      (position_embedding): Embedding(50, 768)\n",
      "    )\n",
      "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): CLIPEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x CLIPEncoderLayer(\n",
      "          (self_attn): CLIPAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): CLIPMLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
      "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPImageProcessorFast, CLIPProcessor, CLIPModel\n",
    "\n",
    "model_name = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "image_processor = CLIPImageProcessorFast.from_pretrained(model_name)    # 이미지 전처리기(리사이즈/정규화 등)\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_name)               # 텍스트 토크나이저\n",
    "model = CLIPModel.from_pretrained(model_name, use_safetensors=True)     # 모델 가중치를 safetensor로 로드\n",
    "\n",
    "print(image_processor)\n",
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaee780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'a photo of a cat',\n",
    "    'a photo of a dog',\n",
    "    'a landscape with mountains'\n",
    "]\n",
    "\n",
    "text_inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors = \"pt\",\n",
    "    padding = True\n",
    ")\n",
    "\n",
    "input_ids = text_inputs['input_ids']\n",
    "attention_mask = text_inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253b6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[ 1.3026,  1.3172,  1.3610,  ...,  1.4632,  1.3464,  1.2880],\n",
      "          [ 1.2880,  1.3026,  1.3172,  ...,  1.4924,  1.4340,  1.3902],\n",
      "          [ 1.3026,  1.3318,  1.3318,  ...,  1.4924,  1.4632,  1.4048],\n",
      "          ...,\n",
      "          [ 0.6311,  0.6457,  0.6895,  ...,  0.5581,  0.4559,  0.7625],\n",
      "          [ 0.6165,  0.6165,  0.6603,  ...,  0.5435,  0.4997,  0.6311],\n",
      "          [ 0.5143,  0.4997,  0.5727,  ...,  0.5143,  0.4705,  0.5435]],\n",
      "\n",
      "         [[ 0.4991,  0.6642,  0.7542,  ...,  1.5496,  1.3995,  1.2945],\n",
      "          [ 0.4841,  0.6491,  0.7392,  ...,  1.5796,  1.5196,  1.4446],\n",
      "          [ 0.4691,  0.6191,  0.6792,  ...,  1.5196,  1.5046,  1.4295],\n",
      "          ...,\n",
      "          [ 0.2289,  0.2289,  0.2740,  ...,  0.1539,  0.0338,  0.2890],\n",
      "          [ 0.1989,  0.1839,  0.2589,  ...,  0.1389,  0.0789,  0.1539],\n",
      "          [ 0.1239,  0.1089,  0.1989,  ...,  0.1539,  0.0338,  0.0789]],\n",
      "\n",
      "         [[-0.4422, -0.3426, -0.1435,  ...,  1.5487,  1.3922,  1.2643],\n",
      "          [-0.4564, -0.2857, -0.0867,  ...,  1.5771,  1.5344,  1.4491],\n",
      "          [-0.4279, -0.2857, -0.1578,  ...,  1.5629,  1.5344,  1.4776],\n",
      "          ...,\n",
      "          [-0.1862, -0.2573, -0.1578,  ..., -0.2146, -0.2146,  0.0555],\n",
      "          [-0.2857, -0.3284, -0.2146,  ..., -0.1435, -0.1720, -0.0582],\n",
      "          [-0.3995, -0.4137, -0.2715,  ..., -0.1293, -0.2289, -0.1009]]]])}\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 이미지 전처리\n",
    "from PIL import Image\n",
    "\n",
    "image_path = 'Golden-Retriever.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "image_inputs = image_processor(\n",
    "    images = image,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "print(image_inputs)\n",
    "\n",
    "pixel_values = image_inputs['pixel_values']\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e60c1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_embed = model.get_image_features(pixel_values = pixel_values)\n",
    "    print(image_embed.pooler_output.shape)\n",
    "\n",
    "    text_embed = model.get_text_features(input_ids = input_ids, attention_mask = attention_mask)\n",
    "    print(text_embed.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69528ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19775248, 0.26828575, 0.17752251]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(image_embed.pooler_output, text_embed.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d1264",
   "metadata": {},
   "source": [
    "- CLIP의 장점\n",
    "    - Zero-shot 분류 가능\n",
    "    - 텍스트 기반 검색 가능 (Image ↔ Text Retrieval)\n",
    "    - 데이터 라벨링 비용 감소\n",
    "    - 멀티모달 통합 표현 학습\n",
    "\n",
    "-  CLIP의 한계\n",
    "    - 세밀한 객체 구분은 어려움 (fine-grained classification)\n",
    "    - 프롬프트에 민감함\n",
    "    - 도메인 특화 데이터에는 성능 저하 가능\n",
    "\n",
    "- CLIP을 활용한 확장 응용\n",
    "    - 이미지 검색 (Text → Image Retrieval)  \n",
    "“a happy dog running on the beach” → 가장 유사한 이미지 검색\n",
    "    - 이미지 설명 평가  \n",
    "생성된 캡션이 이미지와 얼마나 잘 맞는지 점수화\n",
    "\n",
    "    - Stable Diffusion 기반  \n",
    "Stable Diffusion도 내부적으로 CLIP 텍스트 인코더를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1ffff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
